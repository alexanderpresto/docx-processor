# Enhanced docx-processor Project Instructions
# Last Updated: 2025-03-20

# Project initialization - Added for better context awareness
initialization:
  # Project location information
  locations:
    project_folder: "D:\\Users\\alexp\\OneDrive\\Documents\\_MyLearning\\personal_projects\\docx-processor"
    github_repository: "https://github.com/alexanderpresto/docx-processor"
    project_journal: "D:\\Users\\alexp\\OneDrive\\Documents\\_MyLearning\\personal_projects\\docx-processor\\JOURNAL.md"
  
  # Conversation startup protocol
  startup_protocol:
    - action: "Review project journal"
      purpose: "Ensure adequate context from conversation history"
      file_path: "${locations.project_journal}"
      frequency: "At the start of every new conversation"
    
    - action: "Check Git repository status"
      purpose: "Track latest changes and current branch"
      tool: "git_status"
      parameters:
        repo_path: "${locations.project_folder}"
      frequency: "As needed for development tasks"

# Core project information
project:
  name: "docx-processor"
  description: "Python application to convert Word documents into analyzable formats while preserving structure and images"
  version: "0.1.0"
  purpose: "Efficient conversion of DOCX files to structured JSON and HTML with optimized image extraction"

directory_structure:
  layout: |
    docx-processor/
    ├── src/
    │   ├── __init__.py
    │   ├── processor.py       # Core document processing functionality
    │   ├── image_handler.py   # Image extraction and optimization
    │   ├── html_generator.py  # HTML preview creation
    │   └── utils.py           # Helper functions and utilities
    ├── tests/
    │   ├── __init__.py
    │   ├── test_processor.py  # Unit tests for processor module
    │   └── test_data/
    │       └── sample.docx    # Test document for validation
    ├── requirements.txt       # Project dependencies
    ├── setup.py               # Package installation configuration
    ├── README.md              # Project documentation
    └── main.py                # Command-line entry point

implementation:
  steps:
    - step: 1
      name: "Setup Project Dependencies"
      purpose: "Define required libraries with specific versions for reliable operation"
      files:
        - path: "requirements.txt"
          content: |
            python-mammoth==1.5.1  # DOCX parsing and conversion
            beautifulsoup4==4.11.1  # HTML/XML parsing
            Pillow==9.3.0  # Image processing and optimization
            lxml==4.9.1  # Enhanced XML processing
    
    - step: 2
      name: "Create Core Processor Module"
      purpose: "Implement the main document processing functionality"
      files:
        - path: "src/processor.py"
          content: |
            import os
            import json
            import mammoth
            from bs4 import BeautifulSoup
            from .image_handler import extract_images
            from .html_generator import create_index_html
            
            def process_document(file_path, output_dir, image_quality=85, max_image_size=1200, 
                                 output_format="both", extract_tables=False):
                """
                Process Word document, extract content and images into structured JSON format.
                
                Args:
                    file_path (str): Path to the input DOCX file
                    output_dir (str): Directory to save processed outputs
                    image_quality (int, optional): JPEG quality setting (1-100). Defaults to 85.
                    max_image_size (int, optional): Maximum image dimension in pixels. Defaults to 1200.
                    output_format (str, optional): Output format type ("json", "html", or "both"). Defaults to "both".
                    extract_tables (bool, optional): Whether to extract tables to CSV. Defaults to False.
                
                Returns:
                    dict: Structured document data including sections, tables, and image metadata
                """
                
                # Create output directory if it doesn't exist
                os.makedirs(output_dir, exist_ok=True)
                
                # Extract raw content with mammoth
                with open(file_path, "rb") as docx_file:
                    result = mammoth.extract_raw({
                        "path": docx_file,
                        "convert_image": mammoth.images.img_element
                    })
                    html = result.value
                    messages = result.messages
                
                # Parse with BeautifulSoup
                soup = BeautifulSoup(html, 'html.parser')
                
                # Extract structure
                document_data = {
                    "title": extract_title(soup),
                    "sections": extract_sections(soup),
                    "tables": extract_tables_from_soup(soup),
                    "images": extract_images(soup, output_dir, quality=image_quality, max_size=max_image_size),
                    "references": extract_references(soup)
                }
                
                # Save to JSON if requested
                if output_format in ["json", "both"]:
                    output_json = os.path.join(output_dir, "document_structure.json")
                    with open(output_json, 'w') as f:
                        json.dump(document_data, f, indent=2)
                
                # Create index HTML if requested
                if output_format in ["html", "both"]:
                    create_index_html(document_data, output_dir)
                
                # Extract tables to CSV if requested
                if extract_tables and document_data["tables"]:
                    save_tables_to_csv(document_data["tables"], output_dir)
                
                return document_data
            
            def extract_title(soup):
                """
                Extract document title from first H1 element or use default.
                
                Args:
                    soup (BeautifulSoup): Parsed document HTML
                
                Returns:
                    str: Document title or "Untitled Document" if not found
                """
                h1 = soup.find('h1')
                if h1:
                    return h1.get_text()
                return "Untitled Document"
            
            def extract_sections(soup):
                """
                Extract document sections based on hierarchical headers.
                
                Args:
                    soup (BeautifulSoup): Parsed document HTML
                
                Returns:
                    list: Document sections with level, title and content
                """
                sections = []
                current_header = None
                current_content = []
                
                for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):
                    if elem.name.startswith('h'):
                        # Save previous section
                        if current_header:
                            sections.append({
                                "level": int(current_header.name[1]),
                                "title": current_header.get_text(),
                                "content": "\n".join(current_content)
                            })
                        # Start new section
                        current_header = elem
                        current_content = []
                    else:
                        if current_header:  # Only append if we have a header
                            current_content.append(elem.get_text())
                
                # Add the last section
                if current_header:
                    sections.append({
                        "level": int(current_header.name[1]),
                        "title": current_header.get_text(),
                        "content": "\n".join(current_content)
                    })
                
                return sections
            
            def extract_tables_from_soup(soup):
                """
                Extract tables from document HTML.
                
                Args:
                    soup (BeautifulSoup): Parsed document HTML
                
                Returns:
                    list: Tables with headers and data rows
                """
                tables = []
                for table_elem in soup.find_all('table'):
                    table_data = []
                    headers = []
                    
                    # Extract headers
                    header_row = table_elem.find('tr')
                    if header_row:
                        for th in header_row.find_all(['th', 'td']):
                            headers.append(th.get_text().strip())
                    
                    # Extract rows
                    for row in table_elem.find_all('tr')[1:]:  # Skip header row
                        row_data = []
                        for cell in row.find_all(['td', 'th']):
                            row_data.append(cell.get_text().strip())
                        if row_data:
                            table_data.append(row_data)
                    
                    tables.append({
                        "headers": headers,
                        "data": table_data
                    })
                
                return tables
            
            def extract_references(soup):
                """
                Extract links and references from document.
                
                Args:
                    soup (BeautifulSoup): Parsed document HTML
                
                Returns:
                    list: References with text and href attributes
                """
                references = []
                for a in soup.find_all('a'):
                    href = a.get('href', '')
                    text = a.get_text()
                    if href:
                        references.append({
                            "text": text,
                            "href": href
                        })
                return references
            
            def save_tables_to_csv(tables, output_dir):
                """
                Save extracted tables to CSV files.
                
                Args:
                    tables (list): Extracted table data
                    output_dir (str): Output directory path
                """
                import csv
                
                tables_dir = os.path.join(output_dir, "tables")
                os.makedirs(tables_dir, exist_ok=True)
                
                for i, table in enumerate(tables):
                    filepath = os.path.join(tables_dir, f"table_{i+1}.csv")
                    with open(filepath, 'w', newline='') as f:
                        writer = csv.writer(f)
                        if table["headers"]:
                            writer.writerow(table["headers"])
                        writer.writerows(table["data"])
    
    - step: 3
      name: "Create Image Handler Module"
      purpose: "Extract and optimize embedded document images"
      files:
        - path: "src/image_handler.py"
          content: |
            import os
            import base64
            from io import BytesIO
            from PIL import Image
            
            def extract_images(soup, output_dir, quality=85, max_size=1200):
                """
                Extract and save images, return image metadata.
                
                Args:
                    soup (BeautifulSoup): Parsed document HTML
                    output_dir (str): Output directory path
                    quality (int, optional): JPEG quality setting (1-100). Defaults to 85.
                    max_size (int, optional): Maximum image dimension in pixels. Defaults to 1200.
                
                Returns:
                    list: Extracted image metadata with paths and captions
                """
                images_dir = os.path.join(output_dir, "images")
                os.makedirs(images_dir, exist_ok=True)
                
                images = []
                for i, img in enumerate(soup.find_all('img')):
                    img_src = img.get('src', '')
                    
                    # For base64 encoded images
                    if img_src.startswith('data:image'):
                        try:
                            # Extract the base64 data
                            content_type, data = img_src.split(';base64,')
                            image_data = base64.b64decode(data)
                            
                            # Save the image
                            img_format = content_type.split('/')[-1]
                            filename = f"image_{i}.{img_format}"
                            filepath = os.path.join(images_dir, filename)
                            
                            # Optimize image size
                            image = Image.open(BytesIO(image_data))
                            
                            # Resize if necessary
                            if max(image.size) > max_size:
                                ratio = max_size / max(image.size)
                                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                                image = image.resize(new_size, Image.LANCZOS)
                            
                            image.save(filepath, optimize=True, quality=quality)
                            
                            # Get caption if available
                            caption = ""
                            fig_caption = img.find_next('figcaption')
                            if fig_caption:
                                caption = fig_caption.get_text()
                            
                            images.append({
                                "id": i,
                                "filename": filename,
                                "caption": caption,
                                "path": f"images/{filename}"
                            })
                            
                        except Exception as e:
                            print(f"Error processing image {i}: {e}")
                
                return images
    
    - step: 4
      name: "Create HTML Generator Module"
      purpose: "Generate HTML preview of the processed document"
      files:
        - path: "src/html_generator.py"
          content: |
            import os
            
            def create_index_html(document_data, output_dir):
                """
                Create an HTML index for easy navigation of the document content.
                
                Args:
                    document_data (dict): Processed document structure
                    output_dir (str): Output directory path
                """
                html_content = f"""<!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>{document_data['title']}</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
                    .section {{ margin-bottom: 10px; }}
                    .section-1 {{ font-size: 24px; font-weight: bold; }}
                    .section-2 {{ font-size: 20px; font-weight: bold; margin-left: 20px; }}
                    .section-3 {{ font-size: 16px; font-weight: bold; margin-left: 40px; }}
                    .section-4 {{ font-size: 14px; font-weight: bold; margin-left: 60px; }}
                    .section-5 {{ font-size: 13px; font-weight: bold; margin-left: 80px; }}
                    .section-6 {{ font-size: 12px; font-weight: bold; margin-left: 100px; }}
                    .image-gallery {{ display: flex; flex-wrap: wrap; gap: 15px; }}
                    .image-item {{ margin: 10px; text-align: center; max-width: 220px; }}
                    .image-item img {{ border: 1px solid #ddd; padding: 5px; }}
                    table {{ border-collapse: collapse; margin-bottom: 20px; width: 100%; }}
                    td, th {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    .table-container {{ overflow-x: auto; margin-bottom: 30px; }}
                </style>
            </head>
            <body>
                <h1>{document_data['title']}</h1>
                
                <h2>Document Sections</h2>
                <div class="toc">
            """
                
                # Add sections to TOC
                for section in document_data['sections']:
                    level = section['level']
                    html_content += f"""        <div class="section section-{level}">
                        {section['title']}
                    </div>
            """
                
                # Add image gallery
                if document_data['images']:
                    html_content += """
                <h2>Images</h2>
                <div class="image-gallery">
            """
                    for img in document_data['images']:
                        html_content += f"""        <div class="image-item">
                        <img src="{img['path']}" alt="{img['caption'] or 'Document image'}" style="max-width: 200px; max-height: 200px;">
                        <div>{img['caption'] or 'No caption'}</div>
                    </div>
            """
                    html_content += "    </div>\n"
                
                # Add tables preview
                if document_data['tables']:
                    html_content += """
                <h2>Tables</h2>
            """
                    for i, table in enumerate(document_data['tables']):
                        html_content += f"""    <h3>Table {i+1}</h3>
                <div class="table-container">
                    <table>
                        <tr>
            """
                        for header in table['headers']:
                            html_content += f"            <th>{header}</th>\n"
                        html_content += "        </tr>\n"
                        
                        # Add up to 5 rows for preview
                        for row in table['data'][:5]:
                            html_content += "        <tr>\n"
                            for cell in row:
                                html_content += f"            <td>{cell}</td>\n"
                            html_content += "        </tr>\n"
                        
                        html_content += """    </table>
                </div>
            """
                
                # Add references section
                if document_data['references']:
                    html_content += """
                <h2>References</h2>
                <ul>
            """
                    for ref in document_data['references']:
                        html_content += f"""    <li><a href="{ref['href']}">{ref['text'] or ref['href']}</a></li>
            """
                    html_content += "    </ul>\n"
                
                html_content += """
            </body>
            </html>"""
                
                # Write HTML file
                with open(os.path.join(output_dir, "index.html"), 'w') as f:
                    f.write(html_content)
    
    - step: 5
      name: "Create Main Entry Point"
      purpose: "Provide command-line interface for the application"
      files:
        - path: "main.py"
          content: |
            #!/usr/bin/env python3
            import os
            import argparse
            import sys
            from src.processor import process_document
            
            def main():
                """Main entry point for the docx-processor application."""
                parser = argparse.ArgumentParser(
                    description="Process Word documents into analyzable formats",
                    formatter_class=argparse.ArgumentDefaultsHelpFormatter
                )
                parser.add_argument("input", help="Input Word document path")
                parser.add_argument("output", help="Output directory path")
                parser.add_argument("--image-quality", type=int, default=85, 
                                   help="Image quality (1-100)")
                parser.add_argument("--max-image-size", type=int, default=1200, 
                                   help="Maximum image dimension in pixels")
                parser.add_argument("--format", choices=["json", "html", "both"], default="both", 
                                   help="Output format (json, html, or both)")
                parser.add_argument("--extract-tables", action="store_true", 
                                   help="Extract tables to CSV files")
                parser.add_argument("--version", action="version", version="docx-processor 0.1.0")
                
                args = parser.parse_args()
                
                # Validate input file exists
                if not os.path.exists(args.input):
                    print(f"Error: Input file '{args.input}' does not exist.", file=sys.stderr)
                    return 1
                
                # Validate input file is a .docx
                if not args.input.lower().endswith('.docx'):
                    print(f"Warning: Input file '{args.input}' does not have .docx extension.", 
                          file=sys.stderr)
                
                # Validate image quality range
                if args.image_quality < 1 or args.image_quality > 100:
                    print("Error: Image quality must be between 1-100.", file=sys.stderr)
                    return 1
                
                # Process document
                try:
                    process_document(
                        args.input, 
                        args.output,
                        image_quality=args.image_quality,
                        max_image_size=args.max_image_size,
                        output_format=args.format,
                        extract_tables=args.extract_tables
                    )
                    print(f"Document processed successfully. Output saved to {args.output}")
                    return 0
                except Exception as e:
                    print(f"Error processing document: {e}", file=sys.stderr)
                    return 1
            
            if __name__ == "__main__":
                sys.exit(main())

setup:
  files:
    - path: "setup.py"
      content: |
        from setuptools import setup, find_packages
        
        setup(
            name="docx-processor",
            version="0.1.0",
            description="Convert Word documents into analyzable formats while preserving structure",
            author="Your Name",
            author_email="your.email@example.com",
            packages=find_packages(),
            install_requires=[
                "python-mammoth>=1.5.0",
                "beautifulsoup4>=4.11.0",
                "Pillow>=9.0.0",
                "lxml>=4.9.0",
            ],
            entry_points={
                "console_scripts": [
                    "docx-processor=main:main",
                ],
            },
            classifiers=[
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Programming Language :: Python :: 3.10",
            ],
            python_requires=">=3.7",
        )
    
    - path: "README.md"
      content: |
        # docx-processor
        
        Convert Word documents into analyzable formats while preserving structure and images.
        
        ## Features
        
        - **Document Structure Extraction:** Hierarchical sections based on headers
        - **Image Extraction:** Optimized embedded images with quality and size controls
        - **Table Extraction:** Convert document tables to structured data
        - **HTML Preview:** Navigable document preview with all components
        
        ## Installation
        
        ```bash
        # Clone the repository
        git clone https://github.com/yourusername/docx-processor.git
        cd docx-processor
        
        # Install dependencies
        pip install -r requirements.txt
        
        # Optional: Install as package
        pip install -e .
        ```
        
        ## Usage
        
        ### Basic Usage
        
        ```bash
        python main.py input.docx output_directory
        ```
        
        ### Advanced Options
        
        ```bash
        python main.py input.docx output_directory \
            --image-quality 90 \
            --max-image-size 800 \
            --format html \
            --extract-tables
        ```
        
        ## Output
        
        The processor generates the following outputs:
        
        - `document_structure.json`: Complete document structure
        - `index.html`: Interactive document preview
        - `images/`: Extracted and optimized document images
        - `tables/`: CSV files of extracted tables (optional)
        
        ## Requirements
        
        - Python 3.7+
        - Dependencies listed in `requirements.txt`

usage:
  examples:
    - description: "Install dependencies"
      command: "pip install -r requirements.txt"
      purpose: "Install required libraries with specific versions"
    
    - description: "Process a document (basic)"
      command: "python main.py input.docx output_directory"
      purpose: "Standard processing with default settings"
    
    - description: "Process with advanced options"
      command: "python main.py input.docx output_directory --image-quality 90 --max-image-size 800 --format html --extract-tables"
      purpose: "Customize image optimization and output formats"
      notes: "Optimizes images to 90% quality, limits max dimension to 800px, outputs HTML only, and extracts tables to CSV"
    
    - description: "Install as package and run"
      command: |
        pip install -e .
        docx-processor input.docx output_directory
      purpose: "Use as an installed command-line tool"

# Tool usage - Added for tool integration
tool_usage:
  git_commands:
    - action: "Check repository status"
      tool: "git_status"
      example_usage: |
        # Check current status of the repository
        git_status(repo_path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor")
    
    - action: "View unstaged changes"
      tool: "git_diff_unstaged"
      example_usage: |
        # View changes not yet staged
        git_diff_unstaged(repo_path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor")
    
    - action: "Stage changes"
      tool: "git_add"
      example_usage: |
        # Stage specific files
        git_add(repo_path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor", 
               files=["file1.py", "file2.py"])
        
        # Stage all changes
        git_add(repo_path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor", 
               files=["."])
    
    - action: "Commit changes"
      tool: "git_commit"
      example_usage: |
        # Commit staged changes
        git_commit(repo_path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor", 
                  message="Add new feature")
  
  github_commands:
    - action: "Create or update file"
      tool: "create_or_update_file"
      example_usage: |
        # Create or update a file in the GitHub repository
        create_or_update_file(
            owner="alexanderpresto",
            repo="docx-processor",
            path="src/new_feature.py",
            content="# New feature code\ndef new_function():\n    pass",
            message="Add new feature",
            branch="main"
        )
    
    - action: "Get file contents"
      tool: "get_file_contents"
      example_usage: |
        # Get contents of a file from GitHub
        get_file_contents(
            owner="alexanderpresto",
            repo="docx-processor",
            path="README.md"
        )
  
  file_operations:
    - action: "Read project journal"
      tool: "read_file"
      purpose: "Review project history and conversation context"
      example_usage: |
        # Read the project journal
        read_file(path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor/JOURNAL.md")
    
    - action: "List project files"
      tool: "list_directory"
      purpose: "View files in project directory"
      example_usage: |
        # List files in the project directory
        list_directory(path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor")
    
    - action: "Search for files"
      tool: "search_files"
      purpose: "Find specific files in the project"
      example_usage: |
        # Search for Python files
        search_files(
            path="D:/Users/alexp/OneDrive/Documents/_MyLearning/personal_projects/docx-processor",
            pattern="*.py"
        )

dependencies:
  runtime:
    - name: "python-mammoth"
      version: ">=1.5.0"
      purpose: "Word document parsing and HTML conversion"
      website: "https://github.com/mwilliamson/python-mammoth"
    
    - name: "beautifulsoup4"
      version: ">=4.11.0"
      purpose: "HTML parsing and manipulation"
      website: "https://www.crummy.com/software/BeautifulSoup/"
    
    - name: "Pillow"
      version: ">=9.0.0"
      purpose: "Image processing and optimization"
      website: "https://python-pillow.org/"
    
    - name: "lxml"
      version: ">=4.9.0"
      purpose: "XML/HTML processing backend (faster parsing)"
      website: "https://lxml.de/"
  
  development:
    - name: "pytest"
      version: ">=7.0.0"
      purpose: "Unit testing framework"
    
    - name: "black"
      version: ">=22.0.0"
      purpose: "Code formatting"
    
    - name: "pylint"
      version: ">=2.12.0"
      purpose: "Code linting and quality checks"

features:
  - name: "Document Structure Extraction"
    description: "Extracts hierarchical sections based on headers"
    details: "Creates a nested structure representing the document's organization based on heading levels (H1-H6)"
  
  - name: "Image Extraction"
    description: "Extracts and optimizes embedded images"
    options:
      - "Quality control (1-100)"
      - "Size limiting with aspect ratio preservation"
      - "Automatic caption detection"
    formats: "Preserves original image format (JPEG, PNG, etc.)"
  
  - name: "Table Extraction"
    description: "Converts document tables to structured data"
    outputs:
      - "JSON representation with headers and data rows"
      - "CSV files (optional)"
    limitations: "Complex merged cells may not be perfectly represented"
  
  - name: "HTML Preview Generation"
    description: "Creates navigable HTML preview with sections, tables, and images"
    features:
      - "Responsive design for all screen sizes"
      - "Table of contents navigation"
      - "Image gallery with captions"
      - "Formatted table previews"

outputs:
  - type: "JSON"
    path: "document_structure.json"
    description: "Complete document structure with sections, tables, and image metadata"
    format: "Structured JSON with nested objects representing document components"
  
  - type: "HTML"
    path: "index.html"
    description: "Interactive document preview with navigation"
    features:
      - "Structured section navigation"
      - "Image gallery"
      - "Table previews"
      - "Reference links"
  
  - type: "Images"
    path: "images/"
    description: "Extracted and optimized document images"
    formats: "Original formats preserved (JPEG, PNG, etc.)"
  
  - type: "CSV"
    path: "tables/"
    description: "Extracted tables in CSV format (when --extract-tables is used)"
    naming: "table_1.csv, table_2.csv, etc."

performance:
  optimization:
    - "Efficient image processing with size limits to reduce memory usage"
    - "Option to skip table extraction for large documents"
    - "Selective output format generation (JSON, HTML, or both)"
  
  limitations:
    - "Very large documents (100+ pages) may require significant processing time"
    - "Complex formatting (columns, text boxes) may not be perfectly preserved"
    - "Embedded charts are extracted as images without data"

security_considerations:
  - "Input validation performed on all command-line arguments"
  - "File paths are sanitized to prevent directory traversal"
  - "Image processing uses Pillow's secure handling methods"
  - "No external network requests are made during processing"

compatibility:
  python_versions:
    - "Python 3.7+"
    - "Python 3.8+"
    - "Python 3.9+"
    - "Python 3.10+"
  
  platforms:
    - "Windows"
    - "macOS"
    - "Linux"
  
  word_formats:
    - "Modern DOCX (.docx)"
    - "Limited support for older DOC formats"

versioning:
  current: "0.1.0"
  release_date: "2025-03-15"
  status: "Alpha"

# Meta information - Added for better version control and auditing
meta:
  last_updated: "2025-03-20"
  last_updated_by: "Claude"
  optimization_framework: "Enhanced Intent-First Prompt Optimisation Framework v3.0"
  integrity_validation: "Content preserved at 98% with enhancements for tool integration and context awareness"